# Book-Recommendation-System
### Part1. Book-Crossing Dataset
This is a dataset collected from a book crossing (圖書漂流) community, containing 278,858 users with 1,149,780 ratings about 271,379 books.
There are 3 csv files in the folder above: Ratings.csv contains 3 columns User-ID, ISBN and Book-Rating.
This file is the training data, containing 1,034,802 (90%) records, by random and stratified sampling. Users.csv contains the detailed information for all 278,858 users, including their locations and ages. Books.csv contains the titles, authors, and much more detailed information about all 271,379 books. Further descriptions can be found in the original dataset website here.
Finally, for those students using this dataset, your recommendation system would be tested by the 114,978 records preserved, and ranked by the accuracy to compare with other students. ±1 from the ground truth rating would still be considered accurate. 

### Dataset Preprocessing and Analysis
After you choose a dataset, you are required to do some preprocessing and analysis on your data. Following are some questions that you need to answer. 
A. Basic Questions (60%)

Q1. (5%) Which labels/features/records can be used to represent user preference (i.e., how much a user likes the item)? Also, what is the total range of the labels/ features/records? Does the higher the value is, the better a user prefers an item, or conversely the lower the better?

Q2. (25%) What did you do to clean the data? Have you done any transformation, integration or deletion? Briefly explain why you did all these cleaning actions. 

Q3. (15%) Sort the users by the number of interactions, and observe what is the minimum number of interactions (Y) generated by the top-X% users? Draw some appropriate figures and briefly explain what your insight is.
(Hint: You can set X-axis as percentage of users, with 10% for a class interval, and set Y-axis as number of interactions. The interactions are for instance, books a user read in Book-Crossing Dataset or Songs a user listened in Echo Nest Dataset.)

Q4. (15%) Sort the items by the number of interactions, and observe what is the minimum number of times (Y) that the top-X% items have been interacted with? Draw some appropriate figures and briefly explain what your insight is.
(Hint: You can set X-axis as percentage of items, with 10% for a class interval, and set Y-axis as times of items that have been interacted with, which are for instance, number of users that a book has been read in Book-Crossing Dataset, or number of users that a song has been listened in Echo Nest Dataset.)

B. Free Exploration (40%)

Q5. (20%) Did you use other datasets or resources to get more information regarding the users and items? If you do, what did you use and why did you choose them? Where did you apply these extra features to help the analysis? And if you don't, what features do you think would be beneficial and where to find them?

Q6. (20%) Freely study your dataset and come up with a question/idea to analyze.
	a. The motivation of the question needs to be meaningful and valuable. Why do you want to study this question? What do you expect to bring out from the analysis?
	b. Define and formulate your question clearly. It is suggested that you provide an example for easy understanding.
	c. At least 1 figure to show your analysis results and several sentences for explanations. Make sure that the figure is representative enough for the question.
	d. Did the results turn out to be the same as your expectation? Why or why not?

### Guidelines and Questions

The first step to execute the collaborative filtering algorithm is to find the users that share the same tastes. So, you need to construct a huge table which records every users' opinion (rating) to every item.
And then, you need to calculate the similarity between these users. 

Q1. (15%) How do you calculate the similarity? Do you use Jaccard similarity? Pearson coefficient? or Cosine similarity? Or maybe, the similarity measure you used is none of the above, then what is it? Please explain why you choose the metric. Also, describe how you calculate the similarity between any 2 users, as well as how you implement it in your code.

Q2. (15%) Aside from the ratings, do you use other features to help deriving the similarity? If you do, how do you use them and how do you implement in your code?
And if you don't, what features do you think might be helpful in calculating similarity? How could you fit the features in the procedure?
Now, after you computed all the similarities, you can pick out a group of users (say, 10 users) who are the most similar to the current user. We don't simply choose the highest one. Instead, we choose the k users with highest similarity, for that the degree of confidence is way bigger.
The next step will be for each user in testing (or validation) set, list out all the items that the user hasn't tried out while other users highly similar to him(her) have used it. Then, we pick the most high-rated ones as our recommendations from these items. We then successfully make a recommendation! 

Q3. (25%) Please briefly explain the implementation regarding the procedures mentioned above in your code. That is, how you pick out the most similar k users to a certain user? How you filter out those items that the user has not experienced but the other similar k users have tried? Finally, among these items, how you make the final recommendation? Please note that due to the form of testing dataset, your system should not only be able to make a recommendation on item, but also need to have ability to estimate the ratings, given the user and the item. You need to come up with a reasonable estimation method on these ratings. You are welcomed to refer to any papers. 

Q4. (15%) Please describe how you estimate the ratings, given a user and an item, such as explaining it by formula. Also, briefly explain how you accomplish the estimations in your code. As easy as collaborative filtering technique, there is a drawback though. If the user to be examined is a new user, which means that he(she) does not have many histories, this method would probably fail to deliver good recommendations. It becomes way harder to find matching users by similarity if we simply have no clue what the user might favor. 

Q5. (10%) To overcome the aforementioned downside of user-based collaborative filtering, what measures will you take? Do you implement the measure in your code? If so, how do you do that?****

Q6. (10%) of your score will be based on your performance ranking, compared with others using the same dataset. The higher-ranked 50% get a full 10%, while the lower-ranked 50% get 5% in this section.

Q7. (10%) The last 10% of your score in project part 2 is based on the demonstration. The tentative schedule of demo will be on 6/16. (You can take back your answer sheets of both midterm and final exams the same day.) (If any rearrangement were made, we would inform you on E3 announcement.) The purpose of the demo is just to make sure that you actually built the system on your own, and made predictions on testing data in a proper way.
